#!/usr/bin/env python3
"""
P0 Test Suite for GPT-OSS HF Server v4.5.4
Tests all P0 priority improvements
"""

import asyncio
import aiohttp
import json
import time
import hashlib
import sys
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import argparse

# Test configuration
SERVER_URL = "http://localhost:8000"
TEST_TIMEOUT = 30

@dataclass
class TestResult:
    """Test result container"""
    test_name: str
    status: str  # PASS, FAIL, SKIP
    message: str
    duration: float
    details: Optional[Dict] = None

class P0TestSuite:
    """Comprehensive test suite for P0 improvements"""
    
    def __init__(self, server_url: str = SERVER_URL):
        self.server_url = server_url
        self.results = []
        self.session = None
    
    async def setup(self):
        """Setup test session"""
        self.session = aiohttp.ClientSession()
        
        # Wait for server to be ready
        for i in range(10):
            try:
                async with self.session.get(f"{self.server_url}/health") as resp:
                    if resp.status == 200:
                        print("‚úÖ Server is ready")
                        return
            except:
                pass
            await asyncio.sleep(1)
        
        print("‚ùå Server not ready after 10 seconds")
        sys.exit(1)
    
    async def teardown(self):
        """Cleanup test session"""
        if self.session:
            await self.session.close()
    
    async def run_test(self, test_name: str, test_func):
        """Run a single test with timing"""
        print(f"\nüß™ Running: {test_name}")
        start = time.time()
        
        try:
            result = await test_func()
            duration = time.time() - start
            
            if result.get("status") == "PASS":
                print(f"  ‚úÖ PASS ({duration:.2f}s)")
                self.results.append(TestResult(
                    test_name=test_name,
                    status="PASS",
                    message=result.get("message", ""),
                    duration=duration,
                    details=result.get("details")
                ))
            else:
                print(f"  ‚ùå FAIL: {result.get('message')}")
                self.results.append(TestResult(
                    test_name=test_name,
                    status="FAIL",
                    message=result.get("message", "Unknown error"),
                    duration=duration,
                    details=result.get("details")
                ))
        
        except Exception as e:
            duration = time.time() - start
            print(f"  ‚ùå ERROR: {str(e)}")
            self.results.append(TestResult(
                test_name=test_name,
                status="FAIL",
                message=str(e),
                duration=duration
            ))
    
    # ==================== PR-PF01: PromptBuilder Tests ====================
    
    async def test_prompt_determinism(self) -> Dict:
        """Test that same input produces same prompt hash"""
        messages = [
            {"role": "user", "content": "What is 2+2?"}
        ]
        
        hashes = []
        for i in range(3):
            async with self.session.post(
                f"{self.server_url}/v1/chat/completions",
                json={
                    "model": "gpt-oss-20b",
                    "messages": messages,
                    "max_tokens": 10,
                    "temperature": 0,
                    "seed": 42
                }
            ) as resp:
                if resp.status != 200:
                    return {"status": "FAIL", "message": f"Request failed: {resp.status}"}
                
                data = await resp.json()
                # Extract metadata which should contain cache_key
                metadata = data.get("metadata", {})
                cache_key = metadata.get("cache_key", "")
                
                if cache_key:
                    hashes.append(cache_key)
        
        # Check all hashes are identical
        if len(set(hashes)) == 1:
            return {
                "status": "PASS",
                "message": "Deterministic prompt generation confirmed",
                "details": {"hash": hashes[0]}
            }
        else:
            return {
                "status": "FAIL",
                "message": "Non-deterministic prompt generation",
                "details": {"hashes": hashes}
            }
    
    async def test_prompt_version_tagging(self) -> Dict:
        """Test that prompt version is tagged in metadata"""
        async with self.session.post(
            f"{self.server_url}/v1/chat/completions",
            json={
                "model": "gpt-oss-20b",
                "messages": [{"role": "user", "content": "Hello"}],
                "max_tokens": 10
            }
        ) as resp:
            if resp.status != 200:
                return {"status": "FAIL", "message": f"Request failed: {resp.status}"}
            
            data = await resp.json()
            metadata = data.get("metadata", {})
            
            if "prompt_version" in metadata:
                return {
                    "status": "PASS",
                    "message": f"Prompt version tagged: {metadata['prompt_version']}",
                    "details": metadata
                }
            else:
                return {
                    "status": "FAIL",
                    "message": "Prompt version not found in metadata",
                    "details": metadata
                }
    
    async def test_cache_hit_rate(self) -> Dict:
        """Test cache hit rate for repeated requests"""
        messages = [{"role": "user", "content": "Test cache"}]
        
        # Make same request 5 times
        for i in range(5):
            async with self.session.post(
                f"{self.server_url}/v1/chat/completions",
                json={
                    "model": "gpt-oss-20b",
                    "messages": messages,
                    "max_tokens": 10,
                    "temperature": 0,
                    "seed": 1
                }
            ) as resp:
                if resp.status != 200:
                    return {"status": "FAIL", "message": f"Request {i} failed"}
        
        # Check cache stats
        async with self.session.get(f"{self.server_url}/stats") as resp:
            stats = await resp.json()
            prompt_metrics = stats.get("prompt_metrics", {})
            hit_rate = prompt_metrics.get("cache_hit_rate", 0)
            
            if hit_rate >= 0.3:  # 30% hit rate threshold
                return {
                    "status": "PASS",
                    "message": f"Cache hit rate: {hit_rate:.1%}",
                    "details": prompt_metrics
                }
            else:
                return {
                    "status": "FAIL",
                    "message": f"Cache hit rate too low: {hit_rate:.1%}",
                    "details": prompt_metrics
                }
    
    # ==================== PR-ST01: Streaming Tests ====================
    
    async def test_sse_format(self) -> Dict:
        """Test SSE format correctness"""
        async with self.session.post(
            f"{self.server_url}/v1/chat/completions",
            json={
                "model": "gpt-oss-20b",
                "messages": [{"role": "user", "content": "Count to 3"}],
                "max_tokens": 20,
                "stream": True
            }
        ) as resp:
            if resp.status != 200:
                return {"status": "FAIL", "message": f"Stream request failed: {resp.status}"}
            
            events = []
            async for line in resp.content:
                line = line.decode('utf-8').strip()
                if line.startswith("data: "):
                    try:
                        data = json.loads(line[6:])
                        events.append(data)
                    except:
                        pass
            
            # Check for proper event types
            has_start = any(e.get("type") == "start" for e in events)
            has_token = any(e.get("type") == "token" for e in events)
            has_done = any(e.get("type") == "done" for e in events)
            
            if has_start and has_token and has_done:
                return {
                    "status": "PASS",
                    "message": "SSE format correct",
                    "details": {"event_count": len(events)}
                }
            else:
                return {
                    "status": "FAIL",
                    "message": "SSE format incomplete",
                    "details": {
                        "has_start": has_start,
                        "has_token": has_token,
                        "has_done": has_done
                    }
                }
    
    async def test_stream_cancellation(self) -> Dict:
        """Test stream cancellation handling"""
        # Start a stream
        async with self.session.post(
            f"{self.server_url}/v1/chat/completions",
            json={
                "model": "gpt-oss-20b",
                "messages": [{"role": "user", "content": "Tell a long story"}],
                "max_tokens": 500,
                "stream": True
            }
        ) as resp:
            if resp.status != 200:
                return {"status": "FAIL", "message": "Stream start failed"}
            
            # Read a few tokens then cancel
            token_count = 0
            async for line in resp.content:
                line = line.decode('utf-8').strip()
                if line.startswith("data: "):
                    token_count += 1
                    if token_count >= 5:
                        break  # Cancel early
        
        # Check stats for cancellation
        async with self.session.get(f"{self.server_url}/stats") as resp:
            stats = await resp.json()
            
            return {
                "status": "PASS",
                "message": "Stream cancellation handled",
                "details": {
                    "cancelled_total": stats.get("stream_cancelled_total", 0)
                }
            }
    
    # ==================== PR-OBS01: Model Tagging Tests ====================
    
    async def test_model_tagging_health(self) -> Dict:
        """Test model tagging in health endpoint"""
        async with self.session.get(f"{self.server_url}/health") as resp:
            if resp.status != 200:
                return {"status": "FAIL", "message": "Health check failed"}
            
            data = await resp.json()
            model_info = data.get("model", {})
            
            required_tags = ["model_id", "model_size", "dtype", "gpu_mode", "prompt_version"]
            missing = [tag for tag in required_tags if tag not in model_info]
            
            if not missing:
                return {
                    "status": "PASS",
                    "message": "All model tags present in health",
                    "details": model_info
                }
            else:
                return {
                    "status": "FAIL",
                    "message": f"Missing tags: {missing}",
                    "details": model_info
                }
    
    async def test_model_tagging_stats(self) -> Dict:
        """Test model tagging in stats endpoint"""
        # Make a request first
        await self.session.post(
            f"{self.server_url}/v1/chat/completions",
            json={
                "model": "gpt-oss-20b",
                "messages": [{"role": "user", "content": "Test"}],
                "max_tokens": 10
            }
        )
        
        async with self.session.get(f"{self.server_url}/stats") as resp:
            if resp.status != 200:
                return {"status": "FAIL", "message": "Stats request failed"}
            
            data = await resp.json()
            model_info = data.get("model_info", {})
            model_metrics = data.get("model_metrics", {})
            
            if model_info and model_metrics:
                return {
                    "status": "PASS",
                    "message": "Model tagging present in stats",
                    "details": {
                        "model_info": model_info,
                        "metrics_keys": list(model_metrics.keys())
                    }
                }
            else:
                return {
                    "status": "FAIL",
                    "message": "Model tagging incomplete",
                    "details": data
                }
    
    async def test_metrics_endpoint(self) -> Dict:
        """Test Prometheus metrics with model labels"""
        async with self.session.get(f"{self.server_url}/metrics") as resp:
            if resp.status != 200:
                return {"status": "FAIL", "message": "Metrics request failed"}
            
            text = await resp.text()
            
            # Check for key metrics
            has_model_metrics = "model_requests_total" in text
            has_ttft = "ttft_ms" in text
            has_e2e = "e2e_ms" in text
            has_cache = "prompt_cache" in text
            
            if has_model_metrics and has_ttft and has_e2e:
                return {
                    "status": "PASS",
                    "message": "Prometheus metrics properly formatted",
                    "details": {
                        "has_model_metrics": has_model_metrics,
                        "has_cache_metrics": has_cache
                    }
                }
            else:
                return {
                    "status": "FAIL",
                    "message": "Metrics incomplete",
                    "details": {
                        "has_model_metrics": has_model_metrics,
                        "has_ttft": has_ttft,
                        "has_e2e": has_e2e
                    }
                }
    
    # ==================== Performance Tests ====================
    
    async def test_latency_first_profile(self) -> Dict:
        """Test LATENCY_FIRST profile performance"""
        times = []
        
        for i in range(5):
            start = time.time()
            async with self.session.post(
                f"{self.server_url}/v1/chat/completions",
                json={
                    "model": "gpt-oss-20b",
                    "messages": [{"role": "user", "content": "Hello"}],
                    "max_tokens": 50,
                    "profile": "latency_first"
                }
            ) as resp:
                if resp.status != 200:
                    return {"status": "FAIL", "message": f"Request {i} failed"}
                await resp.json()
            
            times.append((time.time() - start) * 1000)
        
        avg_time = sum(times) / len(times)
        p95_time = sorted(times)[int(len(times) * 0.95)]
        
        # Check against SLO: p95 < 7000ms
        if p95_time < 7000:
            return {
                "status": "PASS",
                "message": f"LATENCY_FIRST p95: {p95_time:.0f}ms",
                "details": {
                    "avg_ms": avg_time,
                    "p95_ms": p95_time,
                    "all_times": times
                }
            }
        else:
            return {
                "status": "FAIL",
                "message": f"LATENCY_FIRST too slow: {p95_time:.0f}ms",
                "details": {"times": times}
            }
    
    async def test_cancel_endpoint(self) -> Dict:
        """Test /cancel endpoint"""
        # Start a long request
        request_task = asyncio.create_task(
            self.session.post(
                f"{self.server_url}/v1/chat/completions",
                json={
                    "model": "gpt-oss-20b",
                    "messages": [{"role": "user", "content": "Count to 100"}],
                    "max_tokens": 500,
                    "stream": True
                }
            )
        )
        
        # Wait a bit then try to get request ID from stats
        await asyncio.sleep(0.5)
        
        # For simplicity, just test the endpoint exists
        test_id = "test-123"
        async with self.session.post(f"{self.server_url}/cancel/{test_id}") as resp:
            if resp.status == 200:
                data = await resp.json()
                return {
                    "status": "PASS",
                    "message": "Cancel endpoint working",
                    "details": data
                }
            else:
                return {
                    "status": "FAIL",
                    "message": f"Cancel endpoint returned {resp.status}"
                }
    
    # ==================== Main Test Runner ====================
    
    async def run_all_tests(self):
        """Run all P0 tests"""
        print("\n" + "="*60)
        print("üöÄ GPT-OSS HF Server P0 Test Suite")
        print("="*60)
        
        await self.setup()
        
        # PR-PF01: PromptBuilder Tests
        print("\nüìù PR-PF01: PromptBuilder Tests")
        await self.run_test("Prompt Determinism", self.test_prompt_determinism)
        await self.run_test("Prompt Version Tagging", self.test_prompt_version_tagging)
        await self.run_test("Cache Hit Rate", self.test_cache_hit_rate)
        
        # PR-ST01: Streaming Tests
        print("\nüåä PR-ST01: Streaming Tests")
        await self.run_test("SSE Format", self.test_sse_format)
        await self.run_test("Stream Cancellation", self.test_stream_cancellation)
        
        # PR-OBS01: Model Tagging Tests
        print("\nüè∑Ô∏è PR-OBS01: Model Tagging Tests")
        await self.run_test("Model Tagging (Health)", self.test_model_tagging_health)
        await self.run_test("Model Tagging (Stats)", self.test_model_tagging_stats)
        await self.run_test("Prometheus Metrics", self.test_metrics_endpoint)
        
        # Performance Tests
        print("\n‚ö° Performance Tests")
        await self.run_test("LATENCY_FIRST Profile", self.test_latency_first_profile)
        await self.run_test("Cancel Endpoint", self.test_cancel_endpoint)
        
        await self.teardown()
        
        # Generate report
        self.generate_report()
    
    def generate_report(self):
        """Generate test report"""
        print("\n" + "="*60)
        print("üìä Test Results Summary")
        print("="*60)
        
        passed = sum(1 for r in self.results if r.status == "PASS")
        failed = sum(1 for r in self.results if r.status == "FAIL")
        total = len(self.results)
        
        print(f"\nTotal: {total} | ‚úÖ Passed: {passed} | ‚ùå Failed: {failed}")
        print(f"Success Rate: {(passed/total*100):.1f}%")
        
        if failed > 0:
            print("\n‚ùå Failed Tests:")
            for r in self.results:
                if r.status == "FAIL":
                    print(f"  - {r.test_name}: {r.message}")
        
        # Save detailed report
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total": total,
                "passed": passed,
                "failed": failed,
                "success_rate": passed/total
            },
            "tests": [
                {
                    "name": r.test_name,
                    "status": r.status,
                    "message": r.message,
                    "duration": r.duration,
                    "details": r.details
                }
                for r in self.results
            ]
        }
        
        with open("p0_test_report.json", "w") as f:
            json.dump(report, f, indent=2)
        
        print(f"\nüìÑ Detailed report saved to: p0_test_report.json")
        
        # Check acceptance criteria
        print("\nüéØ P0 Acceptance Criteria:")
        
        criteria = {
            "Prompt Determinism": passed > 0 and any(r.test_name == "Prompt Determinism" and r.status == "PASS" for r in self.results),
            "Prompt Version Tagging": any(r.test_name == "Prompt Version Tagging" and r.status == "PASS" for r in self.results),
            "Cache Hit Rate ‚â•30%": any(r.test_name == "Cache Hit Rate" and r.status == "PASS" for r in self.results),
            "SSE Format Correct": any(r.test_name == "SSE Format" and r.status == "PASS" for r in self.results),
            "Model Tagging Complete": any("Model Tagging" in r.test_name and r.status == "PASS" for r in self.results),
            "p95 TTFT/E2E Within SLO": any(r.test_name == "LATENCY_FIRST Profile" and r.status == "PASS" for r in self.results)
        }
        
        for criterion, met in criteria.items():
            status = "‚úÖ" if met else "‚ùå"
            print(f"  {status} {criterion}")
        
        all_criteria_met = all(criteria.values())
        
        if all_criteria_met:
            print("\nüéâ All P0 acceptance criteria met!")
        else:
            print("\n‚ö†Ô∏è Some P0 criteria not met. Review failed tests.")
        
        return all_criteria_met

async def main():
    parser = argparse.ArgumentParser(description="P0 Test Suite")
    parser.add_argument("--server", type=str, default="http://localhost:8000",
                       help="Server URL")
    args = parser.parse_args()
    
    suite = P0TestSuite(args.server)
    await suite.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())